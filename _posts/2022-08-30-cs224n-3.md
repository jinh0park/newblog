---
layout: post
title:  "강의 요약 - CS224n: Natural Language Processing with Deep Learning (3)"
subtitle: "Lecture 5 : Recurrent Neural Networks and Language Models ~ Lecture 6 : Vanishing Gradients, Fancy RNNs, Seq2Seq"
date:   2022-08-30 19:00:00 +0900
comments: True
---

# I. Contents

# II. Neural dependency parsing

## 1. Neural dependency parsing
Deep learning classifiers are non-linear classifiers (cf. Traditional ML classifiers only give linear decision boundaries)

[A Fast and Accurate Dependency Parser using Neural Networks](https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf) Chen and Manning, 2014

## 2. A bit more about neural networks
- Regularization
- Dropout
- Vectorization
- Non-linearities
- Parameter initialization
- Optimizers
- Learning rates

# III. Language modeling and RNNs

## 1. Language Modeling
**Language Modeling** is the task of predicting what word comes next.

More formally: given a sequence of words \\(x^{(1)},x^{(2)},\cdots,x^{(t)}\\), compute the probability distribution of the next word \\(x^{(t+1)}\\)

\\[P(x^{(t+1)}|x^{(t)},\cdots x^{(1)})\\]

where \\(x^{(t+1)}\\) can be any word in the vocabulary \\(V = {w_1, \cdots , w_{|V|}}\\)

주어진 단어들의 sequence가 있을 때, 그 다음에 올 단어의 확률분포를 구하는 것을 Language modeling이라고 한다.

각 sequence step마다 "단어가 다음에 올 확률"을 곱하면 전체 텍스트의 확률 분포가 되며, 식은 아래와 같다.

\\[P(x^{(1)},\cdots ,x^{(T)})=P(x^{(1)})\times P(x^{(2)|x^(1)}) \times \cdots \\ = \prod_{t=1}^T P(x^{(t)}|x^{(t-1)},\cdots ,x^{(1)}) \\]


## 2. N-gram Language Models
Idea : Collect statistics about how frequent different n-grams are and use these to predict next word.

First we make a **Markov assumption** : \\(x^{(t+1)}\\) depends only on the preceding \\(n-1\\) words

\\[P(x^{(t+1)}|x^{(t)},\cdots x^{(1)}) = P(x^{(t+1)}|x^{(t)},\cdots x^{(t-n+2)})
\\ = \frac {P(x^{(t+1)},x^{(t)},\cdots x^{(t-n+2)})} {P(x^{(t)},\cdots x^{(t-n+2)})}
\\  \approx \frac {count(x^{(t+1)},x^{(t)},\cdots x^{(t-n+2)})} {count(x^{(t)},\cdots x^{(t-n+2)})} \\]

### Problems of N-gram
- Sparsity Problems
  - Problem 1 : 위 식에서 분자 부분의 count가 0이라면, 해당 단어의 확률이 0으로 고정됨 <br/>
    Solution : Add small \\(\delta\\) to the count for every \\(w \in V\\)
  - Problem 1 : 위 식에서 분모 부분의 count가 0이라면, 그 다음 단어의 확률을 정의할 수 없음 <br/>
    Solution : 마지막 단어 하나 생략하고 찾기
- Storage Problems
  - Need to store count for all n-grams you saw in the corpus.
