---
layout: post
title:  "강의 요약 - CS224n: Natural Language Processing with Deep Learning (1)"
subtitle: "Lecture 1 : Introduction and Word Vectors ~ Lecture 2 : Word Vectors 2 and Word Window Classification"
date:   2022-08-27 19:00:00 +0900
comments: True
---

## I. Contents

- Human language and word meaning


## II. Human language and word meaning

### 1. How do we represent the meaning of a word?

#### (1) Definition : **meaning**
Webster 영영사전 : the idea that is represented by a word, phrase, etc.

표준국어대사전("의미") : 말이나 글의 뜻

#### (2) Commonest linguistic way of thinking of meaning :
\\[signifier(symbol) ↔ signified(idea or thing)\\]

#### (3) Common NLP solution
*WordNet*, a thesaurus containing lists of synonym sets and hypernyms

동의어, 유의어(synonyms) 또는 상의어(hypernyms)로 이루어진 사전을 통해 의미를 정의할 수 있다.

※ WordNet의 문제점
- 뉘앙스를 담을 수 없다.
- 단어의 새로운 의미를 담을 수 없다.
- 동의어 등을 판단하는 기준이 주관적이다.
- 제작하는데 인력이 많이 소모된다.
- **Word Similarity를 계산할 수 없다.**

#### (4) Representing words as discrete symbols
"One-hot vector" representing :
\\[motel = [0, 0, 0, 0, 0, 1, 0] \\ hotel = [0, 0, 0, 1, 0, 0, 0]\\]

※ One-hot vector representing의 문제점
모든 vector들이 orthogonal하기 때문에, similarity를 계산할 수 없다.

#### (5) Representing words by their context
**Distributional semantics** : A word's meaning is given by the words that frequency appear close-by.
When a word \\(w\\) appears in a textm its context is the set of words that appears nearby.

특정 단어의 의미는, 글에서 그 특정 단어 주변에 어떤 단어들(=context)이 주로 오는지에 따라 파악할 수 있다.
위 아이디어를 이용하여, 단어를 Vectorize 할 수 있다.

Note : Word vectors are also called word embeddings or word representations, They are a **distributed representation**.

#### (6) Conclusion
NLP에서 단어를 표현하는 방식으로 WordNet, discrete representations, distributed representation 등이 있다.
이때 NLP에서 "유용한" 방식으로 단어를 표현하기 위해서는, 단어 간 유사도(Similarity)를 계산할 수 있도록 distributed representation이 가장 적절한 해법이다.
Distributional semantics에서 착안하여, 단어들을 실수 벡터로 표현하는 방법으로 Word2Vec을 알아본다.

### 2. Word2Vec

#### (1) What is Word2Vec?

**Word2Vec** (Mikolov et al. 2013, [PDF](http://arxiv.org/pdf/1301.3781.pdf)) is a framework for learning word vectors.

![](/img/posts/cs224n/1.PNG){:width="60%" .center-image}

\\[Likelihood = L(\theta) = \prod_{t=1}^{T} \prod_{-m \le j\le m, j\ne0} P(w_{t+j}|w_t;\theta)\\]
\\[objective function = J(\theta) = -\frac{1}{T}log(L(\theta))\\]

특정 단어에 대하여, 주변(Window)에 다른 단어들이 나타날 확률의 곱 (첫번째 \\(\prod\\))을
모든 단어마다 계산해 곱해주면 (두번째 \\(\prod\\)), 이를 Likelihood라고 하며,
-Log를 취하여 objective function을 최소화 함으로써 word representation을 찾을 수 있다.

##### Likelihood란?

관측 데이터 \\(x\\)가 있을 때, 어떤 분포 \\(\theta\\)를 주고, 그 분포에서 데이터가 나왔울 확률을 말한다.
\\[L(\theta|x)=P_{\theta}(X=x)=\prod_{k=1}^n P(x_k|\theta)\\]
Word2Vec에서 \\(\theta\\)란 "to be defined in terms of our vectors", 즉 optimze할 word vector들의 값을 의미하며(이에 따라 분포가 정해진다),
위 Likelihood의 정의로부터 본다면, \\(w_t\\)마다 likelihood를 구한 후 T번 곱하는 것이라고 볼 수 있겠다.


#### (2) How to calculate the probability?
\\[P(o|c) = \frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}\\]
This is an example of the **softmax function**.
The softmax fuction maps arbitrary values \\(x_i\\) to a probability distribution \\(p_i\\)
("max" because amplifies probability to smaller \\(x_i\\), "soft" because still assigns some probability to smaller \\(x_i\\))

#### (3) Optimization

Gradient descent, chain rule, SGD ... (생략)

#### (4) Word2Vec : More details

1. Two model variants
  - Skip-grams (SG) ☞ 위에서 한 내용
  - Continuous Bag of Words (CBOW)
2. Additional efficiency in Training
  - Naive softmax
  - Negative samlping

#### (5) The skip-gram model with negative sampling [PDF](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
Main idea : train binary logistic regressions for a true pair(center word and a word in its context window) versus several noise pairs (the center word paired with a random word)
