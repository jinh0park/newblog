---
layout: post
title:  "강의 요약 - CS224n: Natural Language Processing with Deep Learning (4)"
subtitle: "Lecture 7 : Machine Translation, Attention, Subword Models"
date:   2022-08-31 19:00:00 +0900
comments: True
---


# I. Contents

# II. Machine Translation
Machine Translation is the task of translating a sentence \\(x\\) from one language (**the source language**) to a sentence \\(y\\) in another language (**the target language**).

## 1990s-2010s: Statistical Machine Translation

![](/img/posts/cs224n/15.PNG)

### Alignment

- Q. How to learn translation model \\(P(x|y)\\)?
  - First, need large amount of parallel data e.g. *The Rosetta Stone*
  - Break it down further: Introduce latent \\(a\\) variable into the model: \\(P(x, a|y)\\) where \\(a\\) is the **alignment**, i.e. word-level correspondence between source sentence \\(x\\) and target sentence \\(y\\)
  - Alignments are **latent variables**: They aren't explicitly specified in the data!

언어마다 문법이나 단어 체계가 다르기 때문에, 번역을 하기 위해서는 source sentence와 target sentence 의 단어가 각각 어떻게 대응되는지 파악해야하며, 이를 alignment라고 한다.

Alignment는 one-to-one, many-to-one, one-to-many, many-to-many 등 복잡하게 구성되며, dependency parsing에서의 arc처럼 명시적으로 특정되지 않고 SMT에 내장되므로 **latent variable**이라고 부른다.

### Decoding for SMT

Enumerating every possible \\(y\\) and calculate the probability is too expensive.

Answer : Impose strong independence assumptions in model, use dynamic programming for globally optimal solutions

![](/img/posts/cs224n/16.PNG)

### Conclusion
The best systems of SMT were "extremely complex"


# III. Seq2Seq

## 1. Neural Machine translation
**Neural Machine Translation (NMT)** is a way to do Machine Translation with a *single end-to-end neural network*

The neural network architecture is called a **sequence-to-sequence** model (a.k.a. **seq2seq**) and it involves **two RNNs**

![](/img/posts/cs224n/17.PNG)

- seq2seq is useful for **more than just MT**
  - Summarization
  - Dialogue
  - Parsing
  - Code generation
- seq2seq model is an example of a **Conditional Language Model**

## 2. Training a Neural Machine Translation System

\\[J(\theta) = \frac {1}{T} \sum_{t=1}^T J_t\\]
(\\(J_t\\) is negative lof prob of the word)

seqseq is optimized as a **single system**, Backpropagation operates "end-to-end"

## 3. Multi-layer RNNs
- High-performing RNNs are usually multi-layer : 2 to 4 layers!
- Usually, skip-connections/dense-connections are needed to train deeper RNNs (e.g. 8 layers)
- Transformer-based networks (e.g. BERT) are usually deeper, like 12 or 24 layers.

## 4. Decoding varieties

### Greedy Decoding
- Take most probable word on each step
- **Greedy decoding has no way to undo decisions**

### Exhaustive search Decoding
- Ideally : We could tru computing all possible sequences \\(y\\) and find \\(y\\) that maximizes :
\\[P(y|x)=\prod^T_{t=1}P(y_t|y_1,\cdots ,y_{t-1},x)\\]
- **This \\(O(V^T)\\) complexity is far too expensive!!!**

### Beam search Decoding
- Core idea : On each step of decoder, keep track of the *k most probable partial translations* (which we call **hypotheses**)
- Beam search is not guaranteed to find optimal solution, but much more efficient than exhaustive search.







# IV. Attention
